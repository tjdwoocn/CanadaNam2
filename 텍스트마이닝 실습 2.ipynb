{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### konlpy 사용 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'강산': 1,\n",
       " '길이': 1,\n",
       " '닳': 1,\n",
       " '대한': 2,\n",
       " '도록': 1,\n",
       " '동해': 1,\n",
       " '두산': 1,\n",
       " '마르고': 1,\n",
       " '만세': 1,\n",
       " '무궁화': 1,\n",
       " '물': 1,\n",
       " '백': 1,\n",
       " '보우': 1,\n",
       " '보전': 1,\n",
       " '사람': 1,\n",
       " '삼': 1,\n",
       " '우리나라': 1,\n",
       " '철리': 1,\n",
       " '하느님': 1,\n",
       " '하사': 1,\n",
       " '화려': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Twitter; tw = Twitter()\n",
    "\n",
    "song = '동해물과 백 두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세. 무궁화 삼철리 화려강산 대한 사람, 대한으로 길이 보전하세'\n",
    "word_count = dict()\n",
    "words = tw.nouns(song)\n",
    "for word in words:\n",
    "    if word in word_count:\n",
    "        word_count[word]+= 1\n",
    "    else:\n",
    "        word_count[word]=1\n",
    "        \n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-Norm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc1 euclidean norm : 1.7320508075688772\n",
      "Doc1 normalize: [0.0, 0.5773502691896258, 0.0, 0.5773502691896258, 0.5773502691896258]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Doc1 = {'term1':1, 'term2':1, 'term3':1, 'term4':0, 'term5':0}\n",
    "Doc2 = {'term1':0, 'term2':0, 'term3':2, 'term4':2, 'term5':1}\n",
    "Doc3 = {'term1':2, 'term2':2, 'term3':2, 'term4':0, 'term5':1}\n",
    "\n",
    "Doc1_euclidean_norm = 0\n",
    "sum_value = 0\n",
    "for j in Doc1:\n",
    "    sum_value += math.pow(Doc1[j],2) # 모든 단어에 대한 제곱값을 sum_value에 누적해 더한다.\n",
    "Doc1_euclidean_norm = math.sqrt(sum_value) # sum_value에 루트를 씌워서, 기준값을 완성한다.\n",
    "print('Doc1 euclidean norm :', Doc1_euclidean_norm)\n",
    "\n",
    "Doc1_normalize= []\n",
    "for i in Doc1:\n",
    "    Doc1_normalize.append(Doc1[i]/Doc1_euclidean_norm) # 각 term들에 대해 기준값으로 나눠 단어에 대한 점수를 구한다\n",
    "print('Doc1 normalize:', Doc1_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1, doc2 (inner product) :  2\n",
      "doc1, doc3 (inner product) :  5\n",
      "doc2, doc3 (inner product) :  5\n",
      "0.5285954792089682\n",
      "0.1993592309745642\n",
      "0.4338614829277021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Doc1 = np.array([1,1,1,0,0])\n",
    "Doc2 = np.array([0,0,2,1,1])\n",
    "Doc3 = np.array([2,2,2,0,1])\n",
    "\n",
    "#내적\n",
    "Doc1_dot_Doc2 = np.dot(Doc1, Doc2)\n",
    "print('doc1, doc2 (inner product) : ', Doc1_dot_Doc2)\n",
    "Doc1_dot_Doc3 = np.dot(Doc2, Doc3)\n",
    "print('doc1, doc3 (inner product) : ', Doc1_dot_Doc3)\n",
    "Doc2_dot_Doc3 = np.dot(Doc2, Doc3)\n",
    "print('doc2, doc3 (inner product) : ', Doc2_dot_Doc3)\n",
    "\n",
    "# COS 계산\n",
    "print(1- (Doc1_dot_Doc2/ (np.sqrt(np.sum(np.power(Doc1, 2))) *\n",
    "                         np.sqrt(np.sum(np.power(Doc2,2))))))\n",
    "      \n",
    "print(1- (Doc1_dot_Doc3/ (np.sqrt(np.sum(np.power(Doc1, 2))) *\n",
    "                         np.sqrt(np.sum(np.power(Doc3,2))))))\n",
    "\n",
    "print(1- (Doc1_dot_Doc3/ (np.sqrt(np.sum(np.power(Doc2, 2))) *\n",
    "                         np.sqrt(np.sum(np.power(Doc3,2))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Frequency\n",
    "- 문서의 중요한 단어를 찾기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "docu = {'doc_a' : ['딸기', '안녕', '안녕', '안녕'],\n",
    "        'doc_b' : ['안녕', '안녕', '시금치'],\n",
    "        'doc_c' : ['딸기', '안녕', '사슴', '사슴'],\n",
    "        'doc_d' : ['딸기', '안녕'],\n",
    "        'doc_e' : ['시금치', '안녕']}\n",
    "\n",
    "total_tf_idf = {}\n",
    "\n",
    "for d in docu.keys():\n",
    "    ct = Counter(docu[d])\n",
    "    # 문서에 나타난 빈도 또는 특정한 단어의 빈도수를 전체 단어수로 나눠 나온 단어별 비율\n",
    "    \n",
    "    ### tf  ###\n",
    "    tf = {}\n",
    "    for keyd in ct.keys():\n",
    "        tf[keyd] = ct[keyd]/ len(docu[d])\n",
    "        \n",
    "    ### df  ###\n",
    "    # dict 초기화\n",
    "    df = {}\n",
    "    for cnoun in ct.keys():\n",
    "        df[cnoun] = 0 \n",
    "        \n",
    "    # 전체 문서에서 특정 키워드가 나타나는 횟수\n",
    "    for cnoun in ct.keys():\n",
    "        for keyword in docu.keys():\n",
    "            \n",
    "            # 특정키워드가 있는지를 파악\n",
    "            sp_keyword = set(docu[keyword]) & {cnoun}  # () 안에들어갈 연사자는?  집합간의 연산을 할떄는 & 또는 | 를 사용   \n",
    "\n",
    "            # 특정 키워드가 있으면 1씩 카운트 상승\n",
    "            if sp_keyword != set():\n",
    "                df[cnoun] += 1\n",
    "\n",
    "    tf_idf = {}\n",
    "    for keyword in ct.keys():\n",
    "        #### IDF ####\n",
    "        idf = math.log((len(docu.keys())+1)/ (1+df[keyword]))  # +1을 해준것은 0이 되는것을 방지하기 위해\n",
    "#         idf = math.log(len(docu.keys())/(1+df[keyword]))   # 이 식도 사용이 가능하지만 정보량이 작아 -값이 나오는것\n",
    "                                                            # -가 나올땐 0으로 치환해주는 과정 필요\n",
    "        \n",
    "        ### TF * IDF  ###\n",
    "        tf_idf[keyword] = tf[keyword] * idf\n",
    "    total_tf_idf[d] = tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 문서에서의 단어의 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_a': {'딸기': 0.1013662770270411, '안녕': 0.0},\n",
       " 'doc_b': {'시금치': 0.23104906018664842, '안녕': 0.0},\n",
       " 'doc_c': {'딸기': 0.1013662770270411, '사슴': 0.5493061443340549, '안녕': 0.0},\n",
       " 'doc_d': {'딸기': 0.2027325540540822, '안녕': 0.0},\n",
       " 'doc_e': {'시금치': 0.34657359027997264, '안녕': 0.0}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1823215567939546"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(len(docu.keys())/(1+df['안녕']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soynlp\n",
    "from pprint import pprint\n",
    "from soynlp.tokenizer import MaxScoreTokenizer, RegexTokenizer, LTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['파스타', '가', '좋아', '요']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {'파스':0.3, '파스타':0.7, '좋아요':0.2, '좋아':0.5} # 스코어 지정\n",
    "tokenizer = MaxScoreTokenizer(scores=scores)  # 스코어가 높은것을 기준으로 뽑는 MaxTokenizer\n",
    "tokenizer.tokenize('파스타가좋아요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten=False\n",
      "[[('난', 0, 1, 0.0, 1), ('파스타', 1, 4, 0.7, 3), ('가', 4, 5, 0.0, 1)],\n",
      " [('좋아', 0, 2, 0.5, 2), ('요', 2, 3, 0.0, 1)]]\n",
      "\n",
      "flatten=True\n",
      "['난', '파스타', '가', '좋아', '요']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기가 있는 문장\n",
    "print('flatten=False')\n",
    "pprint(tokenizer.tokenize(\"난파스타가 좋아요\", flatten=False))   # (subword, begin, end, score, 글자수  순)\n",
    "\n",
    "print('\\nflatten=True')\n",
    "pprint(tokenizer.tokenize('난파스타가 좋아요'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "latten = True \n",
      " sent = 데이터마이닝을 공부한다\n",
      "['데이터마이닝', '을', '공부', '한다']\n",
      "\f",
      "latten = True \n",
      " sent = 데이터마이닝을 공부한다\n",
      "[('데이터마이닝', '을'), ('공부', '한다')]\n",
      "\f",
      "latten = True \n",
      " sent = 데이터분석을 위해서 데이터마이닝을 공부한다\n",
      "[('데이터', '분석을'), ('위해서', ''), ('데이터마이닝', '을'), ('공부', '한다')]\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "scores = {'데이':0.5, '데이터':0.5, '데이터마이닝':0.5, '공부':0.5, '공부중':0.45}\n",
    "\n",
    "tokenizer = LTokenizer(scores=scores)\n",
    "\n",
    "print('\\flatten = True \\n sent = 데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부한다'))\n",
    "\n",
    "print('\\flatten = True \\n sent = 데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부한다', flatten = False))\n",
    "\n",
    "print('\\flatten = True \\n sent = 데이터분석을 위해서 데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터분석을 위해서 데이터마이닝을 공부한다', flatten = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolerance = 0.0\n",
      "sent = 데이터마이닝을 공부중이다\n",
      "['데이터마이닝', '을', '공부', '중이다']\n",
      "\n",
      "tolerance = 0.1\n",
      "sent = 데이터마이닝을 공부중이다\n",
      "['데이터마이닝', '을', '공부중', '이다']\n"
     ]
    }
   ],
   "source": [
    "print('tolerance = 0.0\\nsent = 데이터마이닝을 공부중이다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부중이다'))\n",
    "\n",
    "print('\\ntolerance = 0.1\\nsent = 데이터마이닝을 공부중이다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부중이다', tolerance=0.1))  \n",
    "# tolerance (관용, 허용오차)를 주면 너무 빡빡하게 단어를 나누는게 아니고 어느정도 허용어차를 주고 단어선택을 한다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tolerance 는 기존의 스코어에 + 느낌으로 허용해준다고 보면 될듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tolerance=0.1\n",
      "sent=데이터마이닝을 공부중이다\n",
      "['데이터마이닝', '을', '공부', '중이다']\n"
     ]
    }
   ],
   "source": [
    "scores = {'데이':0.5, '데이터':0.5, '데이터마이닝':0.5, '공부':0.6, '공부중':0.45, '분석':0.9}\n",
    "\n",
    "tokenizer = LTokenizer(scores=scores)\n",
    "\n",
    "print('\\ntolerance=0.1\\nsent=데이터마이닝을 공부중이다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부중이다', tolerance=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexTozenizer\n",
    "- 여러 언어가 쓰일때(한국 -> 외래어 -> 숫자등) 그 지점에서 토크나이징 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     이렇게연속된문장은잘리지않습니다만\n",
      " -> ['이렇게연속된문장은잘리지않습니다만']\n",
      "\n",
      "     숫자123이영어abc에섞여있으면ㅋㅋ잘리겠죠\n",
      " -> ['숫자', '123', '이영어', 'abc', '에섞여있으면', 'ㅋㅋ', '잘리겠죠']\n",
      "\n",
      "     띄어쓰기가 포함되어있으면 이정보는10점!꼭띄워야죠\n",
      " -> ['띄어쓰기가', '포함되어있으면', '이정보는', '10', '점', '!', '꼭띄워야죠']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "sents = [\n",
    "    '이렇게연속된문장은잘리지않습니다만',\n",
    "    '숫자123이영어abc에섞여있으면ㅋㅋ잘리겠죠',\n",
    "    '띄어쓰기가 포함되어있으면 이정보는10점!꼭띄워야죠'\n",
    "]\n",
    "\n",
    "for sent in sents:\n",
    "    print('     %s\\n -> %s\\n'  % (sent, tokenizer.tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('띄어쓰기가', ''), ('포함되어', ''), ('있으면', ''), ('이정보는', ''), ('10점!꼭띄워야죠', '')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten = False 띄어쓰기 기준으로 토큰을 나눠서 출력한다\n",
    "tokenizer.tokenize('띄어쓰기가 포함되어 있으면 이정보는 10점!꼭띄워야죠', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cohesion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-7a4e571f7e75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'뉴스의 기사를 이용했던 예시입니다'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mltokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-121-7a4e571f7e75>\u001b[0m in \u001b[0;36mltokenize\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcohesion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cohesion' is not defined"
     ]
    }
   ],
   "source": [
    "def ltokenize(w):\n",
    "    n = len(w)\n",
    "    if n <= 2: return(w,'')\n",
    "    tokens = []\n",
    "    for e in range(2, n+1):\n",
    "        tokens.append(w[:e], w[e:], cohesion(w[:e]))\n",
    "    tokens = sorted(tokens, key=lambda x:-x[2])\n",
    "    return tokens[0][:2]\n",
    "sent = '뉴스의 기사를 이용했던 예시입니다'\n",
    "for word in sent.split():\n",
    "    print(ltokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
